## 02. 컨텐츠 기반 추천

### 컨텐츠 기반 모델

사용자가 이전에 구매한 상품 중에서 좋아하는 상품들과 유사한 상품을 추천하는 방법

#### Represented items

item을 벡터 형태로 표현. 도메인에 따라 다른 방법이 적용.

벡터들간의 유사도를 계산하여서 벡터1 부터 N까지 자신과 유사한 벡터를 추출



#### 장점

- 협업필터링은 다른 사용자들의 평점이 필요한 반면에, 자신의 평점만을 가지고 추천시스템을 만들 수 있음 
- item의 feature를 통해서 추천을 하기에 추천이 된 이유를 설명하기 용이함 
- 사용자가 평점을 매기지 않은 새로운 item이 들어올 경우에도 추천이 가능함 (cold start?)



#### 단점

- item의 feature을 추출해야 하고 이를 통해서 추천하기때문에 제대로 feature을 추출하지 못하면 정확도가 낮음. 그렇기에 Domain Knowledge가 분석시에 필요할 수도 있음 
- 기존의 item과 유사한 item 위주로만 추천하기에 새로운 장르의 item을 추천하기 어려움 
- 새로운 사용자에 대해서 충분한 평점이 쌓이기 전까지는 추천하기 힘듬





### 유사도 함수

> 함수에 따라 유사도 구분이 달라지므로 차이점을 잘 파악하고 사용해야 함 <br>
>
>  강사 曰 상황에 맞게 여러가지 평가 지표를 선택하거나 조합해서 사용(1. 가중치를 주어서 모델 결합, 2. 평가지표 자체를 결합 등)하거나, 고객 그룹별(구매 기록이 많은경우 / 적은경우)로 적용하거나 여러가지 추천 모델을 활용할 수 있음. 

#### 유클리디안 유사도

유클리디안 거리에 역을 취한 값

- 장점 : 계산하기 쉬움
- 단점 : p와 q의 분포가 다르거나 범위가 다른 경우에 상관성을 놓침



#### 코사인 유사도

##### 장점 

벡터의 크기가 중요하지 않은 경우에 거리를 측정하기 위한 매트릭으로 사용. 

(예 : 문서내에서 단어의 빈도수 - 문서들의 길이가 고르지 않더라도 문서내에서 얼마나 나왔는지라는 비율을 확인하기 때문에 상관없음)

##### 단점 

벡터의 크기가 중요한 경우에 대해서 잘 작동하지 않음





represented item을 vectorize 하는 방법 중 item이 텍스트인 경우 TF-IDF 사용

### TF-IDF

특정 문서 내에 특정 단어가 얼마나 자주 등장하는 지를 의미하는 단어 빈도(TF)와 

전체 문서에서 특정 단어가 얼마나 자주 등장하는지를 의미하는 역문서 빈도(DF)를 통해서 

“다른 문서에서는 등장하지 않지만 특정 문서에서만 자주 등장하는 단어＂를 찾아서 문서 내 단어의 가중치를 계산하는 방법

문서의 핵심어를 추출, 문서들 사이의 유사도를 계산, 검색 결과의 중요도를 정하는 작업등에 활용


$$
TF(d, t) * IDF(d, t) = TF-IDF(d, t)
$$


- TF (d, t)

  특정 문서 d에서의 특정 단어 t의 등장 횟수

- DF (t)

  특정 단어 t가 등장한 문서의 수

- IDF (d, t)

  DF(t)에 반비례하는 수

##### 장점

직관적인 해석이 가능

##### 단점

- 대규모 말뭉치를 다룰 때 메모리상의 문제 발생
  - 높은 차원을 가지며 매우 sparse한 형태의 데이터

    예) 100만개의 문서를 다루는 경우 : 100만개의 문서에 등장한 모든 단어를 추출해야하고 이때 단어의 수는 1문서당 새로운 단어가 10개면, 1000만개정도의 말뭉치가 형성됨. 즉, 100만 x 1000만의 매트릭스가 형성

- 한번에 학습 데이터 전체를 진행함 (배치 학습)
  - 큰 작업을 처리하기 어려움 
  - GPU와 같은 병렬처리를 기대하기 힘듬 

- 학습을 통해서 개선하기가 어려움





### Word2Vec

추론 기반의 방법으로 주변 단어(맥락)이 주어졌을 때 무슨단어가 들어갈 지 추측하는 작업

Word2Vec은 단어간 유사도를 반영하여 단어를 벡터로 바꿔주는 임베딩 방법론으로, 원-핫벡터 형태의 sparse matrix 이 가지는 단점을 해소하고자 저차원의 공간에 벡터로 매핑하는 것이 특징. 

Word2Vec은 “비슷한 위치에 등장하는 단어들은 비슷한 의미를 가진다“ 라는 가정을 통해서 학습 진행. 저차원에 학습된 단어의 의미를 분산하여 표현하기에 단어 간 유사도를 계산.

>  추천시스템에서는 단어를 구매상품으로 바꿔서 구매 패턴에 Word2Vec을 적용하여 비슷한 상품을 찾을 수 있다.



#### 알고리즘 - CBOW

CBOW는 **주변에 있는 단어**들을 가지고, **중간에 있는 단어**들을 예측하는 방법입니다. 

input을 두개로 받고 output이 두가지로 나감

1. One Hot Vector 형태의 입력값을 받는다.
2. One Hot Vector 형태의 입력값을 W_in과 곱
3. Hidden state의 값을 W_out과 곱해서 Score를 추출한다.
4. Score에 Softmax를 취해서 각 단어가 나올 확률을 계산한다. 
5.  정답과 Corss Entropy Loss 를 계산
6.  5에서 계산한 Loss를 가지고 Backpropagation 과정을 통해서 Weight를 업데이트
7. 위의 과정을 다른 문맥에 대해서도 수행



#### 알고리즘 - Skip-gram

> CBOW 보다 성능이 좋고 대부분 skip-gram을 사용함

Skip-Gram은 **중간에 있는 단어**로 **주변 단어**들을 예측하는 방법

input은 하나로 받고 output이 두가지로 나감

