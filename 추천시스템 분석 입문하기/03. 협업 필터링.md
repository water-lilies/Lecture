## 03. 협업필터링

### 개요

#### 정의

사용자의 구매 패턴이나 평점을 기반으로 다른 사람들의 구매 패턴, 평점을 통해 추천 하는 방법

사용자의 개인정보나 아이템의 정보가 없어도 추천할 수 있는 것이 장점



#### 종류

- 최근접 이웃기반 
- 잠재 요인기반



#### 장점

- 도메인 지식이 필요하지 않음 
- 사용자의 새로운 흥미를 발견하기 좋음 
- 시작단계의 모델로 선택하기 좋음 (추가적인 문맥정보등의 필요가 없음) 



#### 단점

- 새로운 아이템에 대해서 다루기가 힘듬 
- side features(고객의 개인정보, 아이템의 추가정보)를 포함시키기 어려움





### Neighborhood based method

Neighborhood based Collaborative Filtering은 메모리 기반 알고리즘으로 협업 필터링을 위해 개발된 초기 알고리즘

#### 알고리즘

- User-based collaborative filtering

  사용자의 구매 패턴(평점)과 유사한 사용자를 찾아서 추천 리스트 생성

- Item-based collaborative filtering

  특정 사용자가 준 점수간의 유사한 상품을 찾아서 추천 리스트 생성



#### KNN (K Nearest Neighbors)

가장 근접한 K 명의 Neighbors를 통해서 예측하는 방법

**데이터 (Explicit Feedback)** 유저가 자신의 선호도를 직접 표현한 데이터



#### 장점

- 간단하고 직관적인 접근 방식 때문에 구현 및 디버그가 쉬움 
- 특정 Item을 추천하는 이유를 정당화하기 쉽고 Item 기반 방법의 해석 가능성이 두드러짐
- 추천 리스트에 새로운 item과 user가 추가되어도 상대적으로 안정적



#### 단점

- User 기반 방법의 시간, 속도, 메모리가 많이 필요 
- 희소성 때문에 제한된 범위가 있음 
  -  John의 Top-K 에만 관심이 있음 
  -  John과 비슷한 이웃중에서 아무도 해리포터를 평가하지 않으면, John의 해리포터에 대한 등급 예측을 제공할 수가 없음



### Latent Factor Collaborative Filtering

이웃기반의 협업 필터링은 item space의 벡터와 user space의 벡터를 조합하여  유사도 기반의 추천 진행

잠재요인(Latent Factor) 기반의 협업 필터링은 item과 user의 latent space를 생성하여 둘의 매트릭스 곱을 통해 추천 진행



잠재 요인 협업 필터링은 Rating Matrix에서 빈 공간을 채우기 위해서 사용자와 상품을 잘 표현하는 차원 (Latent Factor)을 찾는 방법. 

행렬 분해는 추천 시스템에서 사용되는 협업 필터링 알고리즘의 한 종류로,  사용자-아이템 상호 작용 행렬을 두 개의 저 차원 직사각형 행렬의 곱으로 분해하여 작동



#### Matrix Factorization Principles

사용자의 잠재요인과 아이템의 잠재요인을 내적해서 평점 매트릭스를 계산



#### SVD(Singular Value Decomposition)

$$
R = U ∑V ^ t
$$

- 고유값 분해(eigen value Decomposition)와 같은 행렬을 대각화 하는 방법
  - U : (m, m), R의 left singular 벡터로 구성된 직교행렬
  - V : (n, n), R의 right singular 벡터로 구성된 직교행렬
  - Σ: (m, n), 주 대각성분이 √λi인 직사각 대각행렬(Singular value)

- 한계

  - 데이터에 결측치가 없어야 함 
  - 대부분의 현업 데이터는 Sparse한 데이터

  

#### SGD(Stochastic Gradient Descent)

고유값 분해(eigen value Decomposition)와 같은 행렬을 대각화 하는 방법

원래 값의 매트릭스와 잠재요인 매트릭스의 차이를  줄이는 *U* 와 *V* 를 찾고자 함



(예시) Explict Feedback 된 형태의 4명의 유저에 대한 3개의 아이템에 대한 평점 Matrix

1. User Latent 와 Item Latent의 임의로 초기화
2. Gradient Descent 진행
3. 모든 평점에 대해서 반복 (epoch : 1)  - ?(알고자하는 값)를 제외한 모든 평점에 대해 진행
4.  2~3의 과정을 10번 반복 (epoch : 10)



- 장점
  - 매우 유연한 모델로 다른 Loss function을 사용할 수 있음
  -  parallelized가 가능함
- 단점
  - 수렴까지 속도가 매우 느림



#### ALS(Alternating Least Squares)

SGD가 두개의 행렬(User Latent, Item Latent)을 동시에 최적화하는 방법이라면, 

ALS는 두 행렬 중 하나를 고정시키고 다른 하나의 행렬을 순차적으로 반복하면서 최적화 하는 방법

기존의 최적화 문제가 convex 형태로 바뀌기에 수렴된 행렬을 찾을 수 있는 장점이 있다.



-  알고리즘
  1. 초기 아이템, 사용자 행렬을 초기화 
  2. 아이템 행렬을 고정하고 사용자 행렬을 최적화 
  3. 사용자 행렬을 고정하고 아이템 행렬을 최적화 
  4. 위의 2, 3 과정을 반복





- 장점
  - SGD보다 수렴속도가 빠름 
  - parallelized가 가능함
- 단점
  - 오직 Loss Squares만 사용가능
